{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain integration ðŸ¦œðŸ”—\n",
    "\n",
    "This notebook shows how to integrate Lit-GPT with LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone Lit-GPT\n",
    "!git clone https://github.com/Lightning-AI/lit-gpt\n",
    "%cd lit-gpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for CUDA\n",
    "!pip install --index-url https://download.pytorch.org/whl/nightly/cu118 --pre 'torch>=2.1.0dev' -q\n",
    "\n",
    "# install the dependencies\n",
    "!pip install .\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_gpt.generate.base import build_llm, generate\n",
    "from lit_gpt import GPT, Tokenizer\n",
    "\n",
    "checkpoint_dir = \"checkpoints/tiiuae/falcon-7b\"\n",
    "devices = 1\n",
    "quantize = \"bnb.int8\"\n",
    "max_new_tokens = 50\n",
    "top_k = 200\n",
    "temperature = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer, fabric = build_llm(checkpoint_dir=checkpoint_dir, devices=devices, quantize=quantize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a (CustomLLM)[https://python.langchain.com/docs/modules/model_io/models/llms/how_to/custom_llm], which is a callable class and it will be responsible for interacting with our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitGPTLLM(LLM):\n",
    "    model: Any\n",
    "    tokenizer: Tokenizer\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"lit-gpt\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        \n",
    "        encoded = self.tokenizer.encode(prompt, device=self.model.device)\n",
    "        prompt_length = encoded.size(0)\n",
    "        max_returned_tokens = prompt_length + max_new_tokens\n",
    "        assert max_returned_tokens <= self.model.config.block_size, (\n",
    "            max_returned_tokens,\n",
    "            self.model.config.block_size,\n",
    "        )  # maximum rope cache length\n",
    "        y = generate(\n",
    "            self.model,\n",
    "            encoded,\n",
    "            max_returned_tokens,\n",
    "            max_seq_length=max_returned_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "        ) \n",
    "        model.reset_cache()\n",
    "        return self.tokenizer.decode(y)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"name\": self.model.config.name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LitGPTLLM(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parrot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
