## Download [phi](https://arxiv.org/abs/2309.05463) weights

### Phi 2

Microsoft Research [released](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) Phi 2, which is a 2.7 billion parameter model trained on "textbook-quality" data with knowledge distillation from Phi 1.5. The model achieves sota results among base LLMs with less than 13B parameters and matches or outperforms models up to 25x larger on complex benchmarks, e.g. it achieves better performance compared to 25x larger Llama-2-70B model on multi-step reasoning tasks, i.e., coding and math. Phi 2 was trained on 1.4T tokens and has not undergone any RLHF alignment nor has it been instruct fine-tuned. Phi 2 shares the same architecture with Phi 1.5 and has context length of 2048 tokens.
The model weights are released under [*Microsoft Research license*](https://huggingface.co/microsoft/phi-2#license).

To download the model weights and convert them to the lit-gpt format, run

```bash
pip install huggingface_hub
python scripts/download.py --repo_id microsoft/phi-2 --from_safetensors True
python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/microsoft/phi-2
```

Inference the model in instruct mode:

```bash
python chat/base.py --checkpoint_dir checkpoints/microsoft/phi-2
```

```
>> Prompt: Write a detailed analogy between mathematics and a lighthouse.

>> Reply: Mathematics is like a lighthouse, guiding and illuminating the path to understanding and knowledge. Just as a lighthouse emits a focused beam of light to guide ships safely to shore, mathematics provides a clear and structured framework for logical reasoning and problem-solving.

Similar to how a lighthouse uses its beam to reveal hidden obstacles and dangers to ships, mathematics allows us to uncover patterns, relationships, and hidden truths. It provides a structured language and tools, such as numbers, equations, and formulas, that help us navigate through complex problems and make sense of the world.

Furthermore, a lighthouse serves as a beacon of hope and direction in times of darkness and uncertainty. Similarly, mathematics offers a sense of certainty and reliability in our pursuit of knowledge. It provides a solid foundation and framework upon which other subjects and disciplines can build, much like how a lighthouse supports and guides ships on their journey.

Moreover, both mathematics and a lighthouse require careful planning, precision, and attention to detail. Just as a lighthouse needs to be strategically placed and calibrated to ensure its beam reaches the desired destination, mathematics requires careful calculation and logical reasoning to reach the correct solution.

In conclusion, mathematics and a lighthouse share the qualities of guiding and illuminating the way, uncovering hidden truths, providing direction, and requiring precision and attention to detail.

Time for inference: 4.97 sec total, 54.09 tokens/sec, 269 tokens
```
or in free generation mode:
```bash
python generate/base.py --prompt "Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?
Bob:" --checkpoint_dir checkpoints/microsoft/phi-2
```
which yields
```
Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?
Bob: Well, one possible reason could be stress. Have you been feeling overwhelmed lately?
Alice: Yes, I've been juggling multiple deadlines and it's been quite taxing.
Carol: Stress can definitely impact your ability to concentrate. Maybe you need
```

### Phi 1.5

A team at Microsoft Research has made available Phi 1.5, which is a 1.3 billion parameter model optimized for common sense reasoning in natural language, showing performance on par with models 5x its size, especially in grade-school mathematics and basic coding. This model retains characteristics of larger LLMs, and significant improvement was noted in reducing toxic and biased generations by avoiding web data. It's also worth highlighting that while this model performs well on language understanding and common sense reasoning tasks, it is a base model that has not undergone any supervised instruction finetuning or finetuning with RLHF.

The model was trained the same data sources (7B tokens) as its [phi-1](https://arxiv.org/abs/2306.11644) predecessor, which includes

- a Python code subset from [The Stack](https://arxiv.org/abs/2211.15533) v1.2
- Q&A texts from [StackOverflow](https://archive.org/download/stackexchange)
- code from DeepMind [code_contests](https://github.com/deepmind/code_contests)
- synthetic Python textbooks and exercises generated by [gpt-3.5-turbo-0301](https://platform.openai.com/docs/models/gpt-3-5)

In addition, to create phi-1.5, the authors included additional textbook-quality synthetic text (roughly 20B tokens) in natural language, which was created using the [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) approach.

The model weights are released under a [*Microsoft Research license*](https://huggingface.co/microsoft/phi-1_5/blob/main/README.md#license).

In order to use the phi-1.5 model checkpoint, which requires about 3 Gb of disk space, download the weights and convert the checkpoint to the lit-gpt format:

```bash
pip install huggingface_hub

python scripts/download.py --repo_id microsoft/phi-1_5

python scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/microsoft/phi-1_5
```

You're done! To execute the model just run:

```bash
pip install tokenizers

python generate/base.py --prompt "Hello, my name is" --checkpoint_dir checkpoints/microsoft/phi-1_5
```
